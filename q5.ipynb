{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152406fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use Naive bayes, K-nearest, and Decision tree classification algorithms to build\n",
    "classifiers on any two datasets. Pre-process the datasets using techniques specified in\n",
    "Q2. Compare the Accuracy, Precision, Recall and F1 measure reported for each dataset\n",
    "using the abovementioned classifiers under the following situations:\n",
    "i. Using Holdout method (Random sampling):\n",
    "a) Training set = 80% Test set = 20%\n",
    "b) Training set = 66.6% (2/3rd of total), Test set = 33.3%\n",
    "ii. Using Cross-Validation:\n",
    "a) 10-fold\n",
    "b) 5-fold'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67705618-f6ff-46b8-94d2-9ee819d2b82c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93dfef3b-5f76-4c13-a647-0fa5632ace71",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy on Iris: 0.7555555555555555\n",
      "K-Nearest Neighbors Accuracy on Iris: 0.6888888888888889\n",
      "Decision Tree Accuracy on Iris: 0.7555555555555555\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X_standardized)\n",
    "\n",
    "# Discretize the data\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "X_binarized = binarizer.fit_transform(X_normalized)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy on Iris:\", accuracy_score(y_test, nb_pred))\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "print(\"K-Nearest Neighbors Accuracy on Iris:\", accuracy_score(y_test, knn_pred))\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy on Iris:\", accuracy_score(y_test, dt_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e4a9726-d90e-432a-8b13-6b15661addae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric for iris\n",
      "Metrics for Iris Dataset:\n",
      "             Algorithm  Accuracy  Precision    Recall  F1 Score\n",
      "0          Naive Bayes  0.755556   0.832371  0.755556  0.689280\n",
      "1  K-Nearest Neighbors  0.688889   0.743965  0.688889  0.699352\n",
      "2        Decision Tree  0.755556   0.832371  0.755556  0.689280\n"
     ]
    }
   ],
   "source": [
    "# Display metrics for iris\n",
    "print(\"metric for iris\")\n",
    "metrics_iris = pd.DataFrame({\n",
    "    'Algorithm': ['Naive Bayes', 'K-Nearest Neighbors', 'Decision Tree'],\n",
    "    'Accuracy': [nb_metrics_iris[0], knn_metrics_iris[0], dt_metrics_iris[0]],\n",
    "    'Precision': [nb_metrics_iris[1], knn_metrics_iris[1], dt_metrics_iris[1]],\n",
    "    'Recall': [nb_metrics_iris[2], knn_metrics_iris[2], dt_metrics_iris[2]],\n",
    "    'F1 Score': [nb_metrics_iris[3], knn_metrics_iris[3], dt_metrics_iris[3]]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "print(\"Metrics for Iris Dataset:\")\n",
    "print(metrics_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1bf2fcc-1ea9-443c-9fa6-f4c5cd4ba4e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d40edc-9cb1-45f0-a57d-fb2fd3f4bab5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy on Titanic: 0.7940074906367042\n",
      "K-Nearest Neighbors Accuracy on Titanic: 0.7940074906367042\n",
      "Decision Tree Accuracy on Titanic: 0.7940074906367042\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Drop rows with missing 'embarked' values\n",
    "df.dropna(subset=['embarked'], inplace=True)\n",
    "\n",
    "# Impute missing 'age' values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
    "df = pd.get_dummies(df, columns=['embarked'], drop_first=True)\n",
    "\n",
    "# Select features and target\n",
    "X = df[['age', 'fare', 'sex', 'embarked_Q', 'embarked_S']]\n",
    "y = df['survived']\n",
    "\n",
    "# Standardize the data\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "X_normalized = normalizer.fit_transform(X_standardized)\n",
    "\n",
    "# Discretize the data\n",
    "X_binarized = binarizer.fit_transform(X_normalized)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Naive Bayes\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy on Titanic:\", accuracy_score(y_test, nb_pred))\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "print(\"K-Nearest Neighbors Accuracy on Titanic:\", accuracy_score(y_test, knn_pred))\n",
    "\n",
    "# Decision Tree\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy on Titanic:\", accuracy_score(y_test, dt_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d48dadd-533c-4efd-9664-650fde61b1ab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric for titanic\n",
      "\n",
      "Metrics for Titanic Dataset:\n",
      "             Algorithm  Accuracy  Precision    Recall  F1 Score\n",
      "0          Naive Bayes  0.794007   0.794437  0.794007  0.794211\n",
      "1  K-Nearest Neighbors  0.794007   0.794437  0.794007  0.794211\n",
      "2        Decision Tree  0.794007   0.794437  0.794007  0.794211\n"
     ]
    }
   ],
   "source": [
    "# Display metrics for titanic dataset\n",
    "\n",
    "print(\"metric for titanic\")\n",
    "\n",
    "# Naive Bayes\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "nb_metrics_titanic = evaluate_model(y_test, nb_pred)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_metrics_titanic = evaluate_model(y_test, knn_pred)\n",
    "\n",
    "# Decision Tree\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "dt_metrics_titanic = evaluate_model(y_test, dt_pred)\n",
    "\n",
    "\n",
    "# Now display metrics for Titanic dataset\n",
    "print(\"\\nMetrics for Titanic Dataset:\")\n",
    "metrics_titanic = pd.DataFrame({\n",
    "    'Algorithm': ['Naive Bayes', 'K-Nearest Neighbors', 'Decision Tree'],\n",
    "    'Accuracy': [nb_metrics_titanic[0], knn_metrics_titanic[0], dt_metrics_titanic[0]],\n",
    "    'Precision': [nb_metrics_titanic[1], knn_metrics_titanic[1], dt_metrics_titanic[1]],\n",
    "    'Recall': [nb_metrics_titanic[2], knn_metrics_titanic[2], dt_metrics_titanic[2]],\n",
    "    'F1 Score': [nb_metrics_titanic[3], knn_metrics_titanic[3], dt_metrics_titanic[3]]\n",
    "})\n",
    "print(metrics_titanic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea695bdc-660d-4aa9-96d3-2160c219d16f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### iris using holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b022bfb9-7d34-43ba-a2da-652d892c1c4d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 80% Training, 20% Test:\n",
      "                     Accuracy  Precision    Recall  F1 Score\n",
      "Naive Bayes          0.766667   0.825299  0.766667  0.709202\n",
      "K-Nearest Neighbors  0.700000   0.765385  0.700000  0.699206\n",
      "Decision Tree        0.766667   0.825299  0.766667  0.709202\n",
      "\n",
      "Results for 66.6% Training, 33.3% Test:\n",
      "                     Accuracy  Precision  Recall  F1 Score\n",
      "Naive Bayes              0.74   0.818713    0.74  0.664154\n",
      "K-Nearest Neighbors      0.74   0.727246    0.74  0.700257\n",
      "Decision Tree            0.74   0.818713    0.74  0.664154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X_standardized)\n",
    "\n",
    "# Discretize the data\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "X_binarized = binarizer.fit_transform(X_normalized)\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    \n",
    "    # Naive Bayes\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    results['Naive Bayes'] = {\n",
    "        'Accuracy': accuracy_score(y_test, nb_pred),\n",
    "        'Precision': precision_score(y_test, nb_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, nb_pred, average='weighted'),\n",
    "        'F1 Score': f1_score(y_test, nb_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    # K-Nearest Neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    results['K-Nearest Neighbors'] = {\n",
    "        'Accuracy': accuracy_score(y_test, knn_pred),\n",
    "        'Precision': precision_score(y_test, knn_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, knn_pred, average='weighted'),\n",
    "        'F1 Score': f1_score(y_test, knn_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_pred = dt.predict(X_test)\n",
    "    results['Decision Tree'] = {\n",
    "        'Accuracy': accuracy_score(y_test, dt_pred),\n",
    "        'Precision': precision_score(y_test, dt_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, dt_pred, average='weighted'),\n",
    "        'F1 Score': f1_score(y_test, dt_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Using Holdout method (Random sampling):\n",
    "# a) Training set = 80%, Test set = 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
    "results_80_20 = evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# b) Training set = 66.6%, Test set = 33.3%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.333, random_state=42)\n",
    "results_66_33 = evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Results for 80% Training, 20% Test:\")\n",
    "print(pd.DataFrame(results_80_20).transpose())\n",
    "\n",
    "print(\"\\nResults for 66.6% Training, 33.3% Test:\")\n",
    "print(pd.DataFrame(results_66_33).transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e569bb-871a-4534-8ea7-bea96a9ae436",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### titanci using holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7843dff7-b967-4067-80eb-ecf0c59883a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 80% Training, 20% Test:\n",
      "                     Accuracy  Precision    Recall  F1 Score\n",
      "Naive Bayes          0.803371   0.806911  0.803371  0.804501\n",
      "K-Nearest Neighbors  0.696629   0.713271  0.696629  0.658234\n",
      "Decision Tree        0.803371   0.806911  0.803371  0.804501\n",
      "\n",
      "Results for 66.6% Training, 33.3% Test:\n",
      "                     Accuracy  Precision    Recall  F1 Score\n",
      "Naive Bayes          0.797980   0.797980  0.797980  0.797980\n",
      "K-Nearest Neighbors  0.525253   0.683961  0.525253  0.495067\n",
      "Decision Tree        0.797980   0.797980  0.797980  0.797980\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Drop rows with missing 'embarked' values\n",
    "df.dropna(subset=['embarked'], inplace=True)\n",
    "\n",
    "# Impute missing 'age' values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
    "df = pd.get_dummies(df, columns=['embarked'], drop_first=True)\n",
    "\n",
    "# Select features and target\n",
    "X = df[['age', 'fare', 'sex', 'embarked_Q', 'embarked_S']]\n",
    "y = df['survived']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X_standardized)\n",
    "\n",
    "# Discretize the data\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "X_binarized = binarizer.fit_transform(X_normalized)\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    \n",
    "    # Naive Bayes\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    results['Naive Bayes'] = evaluate_model(y_test, nb_pred)\n",
    "    \n",
    "    # K-Nearest Neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    results['K-Nearest Neighbors'] = evaluate_model(y_test, knn_pred)\n",
    "    \n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_pred = dt.predict(X_test)\n",
    "    results['Decision Tree'] = evaluate_model(y_test, dt_pred)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Using Holdout method (Random sampling):\n",
    "# a) Training set = 80%, Test set = 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
    "results_80_20 = evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# b) Training set = 66.6%, Test set = 33.3%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.333, random_state=42)\n",
    "results_66_33 = evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Results for 80% Training, 20% Test:\")\n",
    "print(pd.DataFrame(results_80_20, index=['Accuracy', 'Precision', 'Recall', 'F1 Score']).transpose())\n",
    "\n",
    "print(\"\\nResults for 66.6% Training, 33.3% Test:\")\n",
    "print(pd.DataFrame(results_66_33, index=['Accuracy', 'Precision', 'Recall', 'F1 Score']).transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6793eed-bd37-4b99-abbe-3939d8e2c63e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### iris using k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83b2b485-3fc0-4e63-8e36-01a4d50f346b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Accuracy with 10-fold Cross-Validation: 0.7933333333333333\n",
      "GaussianNB Accuracy with 5-fold Cross-Validation: 0.7933333333333332\n",
      "KNeighborsClassifier Accuracy with 10-fold Cross-Validation: 0.64\n",
      "KNeighborsClassifier Accuracy with 5-fold Cross-Validation: 0.6399999999999999\n",
      "DecisionTreeClassifier Accuracy with 10-fold Cross-Validation: 0.7866666666666667\n",
      "DecisionTreeClassifier Accuracy with 5-fold Cross-Validation: 0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X_standardized)\n",
    "\n",
    "# Discretize the data\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "X_binarized = binarizer.fit_transform(X_normalized)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to perform cross-validation and print results\n",
    "def cross_validation(clf, X, y, cv_folds):\n",
    "    scores = cross_val_score(clf, X, y, cv=cv_folds, scoring='accuracy')\n",
    "    print(f\"{clf.__class__.__name__} Accuracy with {cv_folds}-fold Cross-Validation:\", scores.mean())\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "cross_validation(nb, X_binarized, y, 10)  # 10-fold cross-validation\n",
    "cross_validation(nb, X_binarized, y, 5)   # 5-fold cross-validation\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "cross_validation(knn, X_binarized, y, 10)  # 10-fold cross-validation\n",
    "cross_validation(knn, X_binarized, y, 5)   # 5-fold cross-validation\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "cross_validation(dt, X_binarized, y, 10)  # 10-fold cross-validation\n",
    "cross_validation(dt, X_binarized, y, 5)   # 5-fold cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f788501-e3cd-41d7-9601-514f8f04c9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### titanic using k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ac9e3e8-63a4-41ac-aab6-c028b83049d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Accuracy with 10-fold Cross-Validation: 0.7862487231869254\n",
      "GaussianNB Accuracy with 5-fold Cross-Validation: 0.7806640005078398\n",
      "KNeighborsClassifier Accuracy with 10-fold Cross-Validation: 0.7626021450459652\n",
      "KNeighborsClassifier Accuracy with 5-fold Cross-Validation: 0.7367866438138767\n",
      "DecisionTreeClassifier Accuracy with 10-fold Cross-Validation: 0.7873723186925433\n",
      "DecisionTreeClassifier Accuracy with 5-fold Cross-Validation: 0.7874055735415476\n",
      "Naive Bayes Accuracy on Titanic: 0.7940074906367042\n",
      "K-Nearest Neighbors Accuracy on Titanic: 0.7940074906367042\n",
      "Decision Tree Accuracy on Titanic: 0.7940074906367042\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "# Drop rows with missing 'embarked' values\n",
    "df.dropna(subset=['embarked'], inplace=True)\n",
    "\n",
    "# Impute missing 'age' values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
    "df = pd.get_dummies(df, columns=['embarked'], drop_first=True)\n",
    "\n",
    "# Select features and target\n",
    "X = df[['age', 'fare', 'sex', 'embarked_Q', 'embarked_S']]\n",
    "y = df['survived']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "normalizer = MinMaxScaler()\n",
    "X_normalized = normalizer.fit_transform(X_standardized)\n",
    "\n",
    "# Discretize the data\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "X_binarized = binarizer.fit_transform(X_normalized)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Function to perform cross-validation and print results\n",
    "def cross_validation(clf, X, y, cv_folds):\n",
    "    scores = cross_val_score(clf, X, y, cv=cv_folds, scoring='accuracy')\n",
    "    print(f\"{clf.__class__.__name__} Accuracy with {cv_folds}-fold Cross-Validation:\", scores.mean())\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "cross_validation(nb, X_binarized, y, 10)  # 10-fold cross-validation\n",
    "cross_validation(nb, X_binarized, y, 5)   # 5-fold cross-validation\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "cross_validation(knn, X_binarized, y, 10)  # 10-fold cross-validation\n",
    "cross_validation(knn, X_binarized, y, 5)   # 5-fold cross-validation\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "cross_validation(dt, X_binarized, y, 10)  # 10-fold cross-validation\n",
    "cross_validation(dt, X_binarized, y, 5)   # 5-fold cross-validation\n",
    "\n",
    "# Existing code to evaluate on single train-test split for comparison\n",
    "# Naive Bayes\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy on Titanic:\", accuracy_score(y_test, nb_pred))\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "print(\"K-Nearest Neighbors Accuracy on Titanic:\", accuracy_score(y_test, knn_pred))\n",
    "\n",
    "# Decision Tree\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy on Titanic:\", accuracy_score(y_test, dt_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bae27-9421-40c2-98c6-6f49832debe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
